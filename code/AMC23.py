# AMC23
# 26 / 40  65%
# qwen2.5-7b
import os
import re
import time
import gc
from tqdm import tqdm
import json
import torch
from modelscope import AutoModelForCausalLM, AutoTokenizer
from collections import Counter

# ============================================================================
# 1. SETUP & INITIALIZATION (Environment Setup)
# ============================================================================

def setup_modelscope():
    """Setup ModelScope environment for AutoDL"""
    os.makedirs("./modelscope", exist_ok=True)
    os.makedirs("./results", exist_ok=True)
    
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

class QwenModelScope:
    def __init__(self):
        setup_modelscope()
        model_id = 'qwen/Qwen2.5-7B-Instruct'
        
        print("Loading Qwen 2.5-7B from ModelScope...")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True,
            cache_dir="./modelscope"
        )
        
        # Load model with optimizations for 4090
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            cache_dir="./modelscope",
            low_cpu_mem_usage=True,
            use_cache=True,
        )
        
        self.model.eval()
        gc.collect()
        torch.cuda.empty_cache()
        print(f"Model loaded successfully! GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.1f} GB")
    
    def generate(self, messages, temperature=0.0, max_tokens=8192, top_p=0.9):
        text = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            text, 
            return_tensors="pt",
            truncation=True,
            max_length=4096
        ).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=max(temperature, 0.001),
                top_p=top_p,
                do_sample=temperature > 0,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True,
                repetition_penalty=1.05
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        del inputs, outputs
        torch.cuda.empty_cache()
        return response.strip()

print("Initializing Qwen client with ModelScope...")
client = QwenModelScope()

# ============================================================================
# 2. UTILS: PARSING & ROBUST NORMALIZATION (Updated for AMC23 format)
# ============================================================================

def safe_generate(messages, **kwargs):
    try:
        return client.generate(messages, **kwargs)
    except torch.cuda.OutOfMemoryError:
        print("GPU OOM detected, cleaning memory...")
        torch.cuda.empty_cache()
        gc.collect()
        time.sleep(1)
        return client.generate(messages, **kwargs)
    except Exception as e:
        print(f"Generation error: {e}")
        torch.cuda.empty_cache()
        gc.collect()
        return ""

def parse_numeric_answer(text: str) -> str:
    if not text: return ""
    # 优先提取 \boxed{} 中的内容
    boxed_matches = re.findall(r"\\boxed\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}", text)
    if boxed_matches:
        return boxed_matches[-1].strip()
    
    # 其次尝试提取 "The answer is" 后面的内容
    text_lower = text.lower()
    if "answer is" in text_lower:
        after = text[text_lower.rfind("answer is") + 9:]
        # 提取直到行尾或句号
        line_end = after.split('\n')[0].split('.')[0]
        return line_end.strip()
    
    # 如果没有明确标记，对于只有一行的输出直接返回
    # 对于多行，尝试返回最后一行（通常是答案）
    lines = text.strip().split('\n')
    if lines:
        # 简单的 heuristic: 最后一个看起来像数字或短文本的行
        return lines[-1].strip()
        
    return text

def normalize_answer(text: str) -> str:
    """
    对答案字符串进行清洗和标准化：
    1. 去除 LaTeX 格式 (\boxed, \frac 等)
    2. 去除单位
    3. 关键修改：将 "27.0" 这种格式统一转换为 "27"，以便与整数答案匹配
    """
    if not text: return ""
    text = str(text).strip()
    
    # 1. 去除 LaTeX \boxed 外壳
    if text.startswith(r"\boxed{") and text.endswith("}"):
        text = text[7:-1]
    
    # 2. 替换常见 LaTeX 命令
    text = re.sub(r"\\frac\{([^{}]+)\}\{([^{}]+)\}", r"\1/\2", text) # \frac{a}{b} -> a/b
    text = re.sub(r"\\binom\{([^{}]+)\}\{([^{}]+)\}", r"binom(\1,\2)", text) 
    text = re.sub(r"\\sqrt\{([^{}]+)\}", r"sqrt(\1)", text)
    text = re.sub(r"\\cdot", "*", text)
    text = re.sub(r"\\times", "*", text)
    
    # 3. 去除单位和符号
    text = re.sub(r"\^\{\\circ\}", "", text) 
    text = re.sub(r"\^\\circ", "", text)
    text = text.replace("°", "").replace("degrees", "")
    text = text.replace(r"\%", "/100") 
    text = text.replace(r"\$", "").replace("$", "")
    text = text.replace("units", "").replace("sq units", "")
    
    # 4. 去除排版类命令
    text = text.replace(r"\displaystyle", "")
    text = text.replace(r"\text", "")
    text = text.replace(r"\left", "").replace(r"\right", "")
    text = text.replace(r"\quad", "")
    
    # 5. 保留括号，但去除所有空白字符
    text = "".join(text.split())
    
    # 6. 去除末尾标点
    if text.endswith('.') or text.endswith(','):
        text = text[:-1]

    # --- 关键新增: 处理 AMC23 的 .0 格式 ---
    # 如果字符串以 .0 或 .00 结尾，直接去掉，变成整数形式
    # 例如: "27.0" -> "27", "3159.0" -> "3159", "-1.0" -> "-1"
    if '.' in text:
        text = re.sub(r"\.0+$", "", text)
        
    return text

def is_math_equivalence(pred_str: str, gt_str: str) -> bool:
    """
    判断两个答案在数学上是否等价 (处理格式差异、浮点误差、集合顺序)
    核心逻辑：数值相同即判定为正确
    """
    # 0. 预处理：尝试直接将两个字符串都转为 float 进行对比
    # 这是处理 "27" vs "27.0" 最直接有效的方法
    try:
        # 清理掉可能影响 float 转换的非数字字符 (保留 ., -, 数字)
        def clean_to_float(s):
            # 简单提取数字部分，处理 LaTeX 可能残留的符号
            clean = re.sub(r"[^\d\.\-]", "", s)
            return float(clean)

        f_pred = clean_to_float(pred_str)
        f_gt = clean_to_float(gt_str)
        
        # 只要数值差距极小，直接返回 True
        if abs(f_pred - f_gt) < 1e-4:
            return True
    except:
        pass

    # 如果直接 float 转换失败（例如包含 sqrt, 分数等），则进行标准化流程
    norm_pred = normalize_answer(pred_str)
    norm_gt = normalize_answer(gt_str)
    
    # 1. 字符串直接匹配 (标准化后，"27.0" 已变为 "27")
    if norm_pred == norm_gt:
        return True
    
    # 2. 复杂数值匹配 (使用 eval 处理分数、算式)
    try:
        def safe_eval(s):
            # 替换 LaTeX/数学符号为 Python 运算符
            s_clean = s.replace("^", "**").replace("{", "(").replace("}", ")")
            # 允许的字符：数字, ., /, -, +, *, e, (, )
            if not re.match(r'^[\d\.\/\+\-\*e\(\)]+$', s_clean):
                raise ValueError
            return eval(s_clean, {"__builtins__": None}, {})

        val_pred = safe_eval(norm_pred)
        val_gt = safe_eval(norm_gt)
        
        if abs(val_pred - val_gt) < 1e-4:
            return True
    except:
        pass

    # 3. 集合/列表匹配 (处理顺序不同，例如 "1, 2" == "2, 1")
    if ',' in norm_pred and ',' in norm_gt and '(' not in norm_pred and '(' not in norm_gt:
        try:
            set_pred = sorted([normalize_answer(x) for x in norm_pred.split(',') if x])
            set_gt = sorted([normalize_answer(x) for x in norm_gt.split(',') if x])
            if set_pred == set_gt:
                return True
        except:
            pass
            
    # 4. 坐标集合匹配
    if ')' in norm_pred and ')' in norm_gt:
        try:
            items_pred = sorted(re.findall(r'\([^\)]+\)', norm_pred))
            items_gt = sorted(re.findall(r'\([^\)]+\)', norm_gt))
            if items_pred and items_gt and items_pred == items_gt:
                return True
        except:
            pass

    return False

# ============================================================================
# 3. DATASET LOADING (AMC23.JSONL) - MODIFIED
# ============================================================================

def create_sample_dataset():
    print("Using built-in sample problems for testing...")
    sample_problems = [
        {"problem": "If $3x + 7 = 22$, what is the value of $x$?", "solution": "5", "level": "4", "type": "Algebra"},
        {"problem": "Find the sum of all positive integers $n$ such that $1 \\leq n \\leq 100$ and \\gcd(n, 6) = 1$.", "solution": "1633", "level": "5", "type": "Number Theory"},
    ]
    return sample_problems, []

def load_real_math_dataset():
    # --- 关键修改：文件路径改为 AMC23.jsonl ---
    dataset_file = "./AMC23.jsonl"
    
    if not os.path.exists(dataset_file):
        print(f"Warning: {dataset_file} not found.")
        return create_sample_dataset()
    
    data_list = []
    try:
        with open(dataset_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    line = line.strip()
                    if not line: continue
                    data = json.loads(line)
                    
                    # 1. 映射 Question
                    if 'question' in data:
                        data['problem'] = data['question']
                    
                    # 2. 映射 Answer
                    # AMC23 数据中 answer 字段可能是数字(如 27.0)，需转字符串
                    if 'answer' in data:
                        data['solution'] = str(data['answer'])
                    # 兼容性保留
                    elif 'final_answer' in data and isinstance(data['final_answer'], list):
                        if len(data['final_answer']) > 0:
                            data['solution'] = str(data['final_answer'][0])
                        else:
                            data['solution'] = ""
                    elif 'solution' in data:
                        if isinstance(data['solution'], list):
                            data['solution'] = "\n".join(data['solution'])
                        else:
                            data['solution'] = str(data['solution'])

                    # 3. 补充默认 Type
                    if 'type' not in data:
                        if 'subfield' in data:
                            data['type'] = data['subfield']
                        elif 'subject' in data:
                            data['type'] = data['subject']
                        else:
                            data['type'] = "General Math"
                    
                    data_list.append(data)
                except Exception as e: 
                    continue
    except Exception as e:
        print(f"Error reading dataset: {e}")
        return create_sample_dataset()

    if not data_list: 
        return create_sample_dataset()
    
    return [], data_list

# ============================================================================
# 4. PIPELINE: BASELINE, VOTING & EXTRACTION
# ============================================================================

def generate_baseline_answer(question: str) -> str:
    messages = [
        {"role": "system", "content": "You are a math solver. Provide clear reasoning and place the final numeric answer at the end."},
        {"role": "user", "content": f"Solve this problem: {question}"}
    ]
    return safe_generate(messages, temperature=0.0)

def generate_diverse_answers(question: str, n: int = 3) -> list[str]:
    prompts = [
        """You are a mathematics expert. Solve using algebraic manipulation.""",
        """You are a strategic problem solver. Test edge cases and boundaries.""",
        """You are a visual mathematician. Use geometric intuition or structure."""
    ]
    answers = []
    for i in range(n):
        messages = [
            {"role": "system", "content": prompts[i % len(prompts)]},
            {"role": "user", "content": f"Solve this problem: {question}"}
        ]
        answers.append(safe_generate(messages, temperature=0.7, top_p=0.9)) 
    return answers

def select_solution_by_voting(answers: list[str]) -> str:
    """
    3-out-of-2 Voting Mechanism
    """
    if not answers:
        return ""
    
    parsed_answers = []
    print(f"   - [Voting] Analyzing {len(answers)} experts...")
    
    for idx, ans_text in enumerate(answers):
        raw_val = parse_numeric_answer(ans_text)
        norm_val = normalize_answer(raw_val)
        parsed_answers.append(norm_val)
        display_val = norm_val if norm_val else "[No Answer]"
        print(f"     > Expert {idx+1}: {display_val}")
    
    counts = Counter(parsed_answers)
    
    if not counts:
        return answers[0]
        
    top_answer, count = counts.most_common(1)[0]
    final_solution_text = answers[0] # Default Fallback
    
    if count >= 2:
        if top_answer == "":
            print(f"     > Consensus reached on EMPTY answer. (Fallback to Exp 1 text).")
        else:
            print(f"     > Consensus Reached: Answer '{top_answer}' (Votes: {count}/3)")
            # Find the first expert text that matches the winner
            for i, val in enumerate(parsed_answers):
                if val == top_answer:
                    final_solution_text = answers[i]
                    break
    else:
        print(f"     > No Consensus (All distinct). Fallback to Expert 1.")
    
    return final_solution_text

def extract_steps_with_quad_cards(answers: list[str], question: str) -> list[dict]:
    print("   - [Step Extraction] Using selected solution for Quad Card generation...")
    
    best_answer = answers[0] if answers else ""
    
    messages = [
        {"role": "system", "content":
            "You are a Mathematical Logic Analyzer. \n"
            "Task: Break down the provided Reference Solution into clear, distinct logical steps.\n"
            "RULES:\n"
            "1. Quad Cards: For each step, define 4 distinct ways to describe the logic (Cards A/B/C/D).\n"
            "2. Sequence: Ensure the steps flow logically from start to finish.\n\n"
            "FORMAT:\n"
            "Step 1: || Card A: [Logic 1] || Card B: [Logic 2] || Card C: [Logic 3] || Card D: [Logic 4] || Math: [Equation/Result]\n"
            "Step 2: || ...\n"
        },
        {"role": "user", "content": 
            f"Question: {question}\n\nReference Solution:\n{best_answer}\n\n"
            "Extract the Step-by-Step execution plan in the Quad Card format."
        }
    ]
    
    response = safe_generate(messages, temperature=0.1)
    
    extracted_steps = []
    for line in response.split('\n'):
        line = line.strip()
        if "||" in line and (line.startswith("Step") or line[0].isdigit()):
             extracted_steps.append(line)
             
    return extracted_steps

# ============================================================================
# 5. PIPELINE: MUTATION & VERIFICATION LOOP
# ============================================================================

def _verify_mutation_quality(original_q: str, mutated_q: str) -> tuple[bool, str]:
    messages = [
        {"role": "system", "content": 
            "You are a Strict Math Logic Auditor. Your job is to crash-test a math problem for contradictions.\n"
            "Analyze the 'Mutated Problem' for the following FATAL FLAWS:\n"
            "1. Domain Contradictions: (e.g., Probability > 1, Negative Length/Time/Count).\n"
            "2. Geometry Violations: (e.g., Triangle inequality a+b<=c, Sum of angles != 180).\n"
            "3. Algebra Impossibilities: (e.g., Sqrt of negative, Denominator is zero, Discriminant < 0 for real roots).\n"
            "4. Integer Constraints: (e.g., Problem implies 'people' or 'cars' but equations yield 3.5).\n"
            "5. Logical Consistency: Do the new numbers make the premises mutually exclusive?\n\n"
            "Output Format:\n"
            "ANALYSIS: [Brief thought process about constraints]\n"
            "RESULT: [PASS or FAIL]\n"
            "REASON: [If FAIL, explain the contradiction]"
        },
        {"role": "user", "content": 
            f"Original Problem (Reference): {original_q}\n\n"
            f"Mutated Problem (Target): {mutated_q}\n\n"
            "Check: Is the Mutated Problem mathematically solvable and strictly logical?"
        }
    ]
    
    response = safe_generate(messages, temperature=0.1)
    
    if "RESULT: PASS" in response:
        return True, "Valid"
    else:
        reason = "Unknown Logic Error"
        if "REASON:" in response:
            reason = response.split("REASON:")[-1].strip()
        elif "ANALYSIS:" in response:
            reason = response.split("ANALYSIS:")[-1].split("RESULT:")[0].strip()
        return False, reason

def generate_mutated_variant(question: str) -> str:
    feedback = "" 
    for attempt in range(3): 
        prompt_content = (
            "You are a Math Problem Generator. Create a 'Mutated Variant' of the user's problem.\n"
            "RULES:\n"
            "1. Keep the logic/structure EXACTLY the same. DO NOT change the question type.\n"
            "2. Change numbers to REASONABLE values. \n"
            "   - If original result was an integer, try to ensure new result is likely an integer (or simple fraction).\n"
            "   - Ensure geometry constraints hold (e.g., small side changes).\n"
            "3. Output ONLY the new problem text."
        )
        
        if feedback:
            prompt_content += f"\n\n[WARNING] Previous attempt failed because: {feedback}. Please fix this in the new generation."

        messages = [
            {"role": "system", "content": prompt_content},
            {"role": "user", "content": f"Original Problem: {question}"}
        ]
        
        temp = 0.7 if attempt == 0 else 0.4
        candidate = safe_generate(messages, temperature=temp).strip()
        
        if ":" in candidate and len(candidate.split(":")[0]) < 20:
             if "Problem" in candidate.split(":")[0]:
                 candidate = candidate.split(":", 1)[1].strip()

        is_valid, reason = _verify_mutation_quality(question, candidate)
        
        if is_valid:
            print(f"    > [Mutation] Attempt {attempt+1} Success.")
            return candidate
        else:
            print(f"    > [Mutation] Attempt {attempt+1} Rejected: {reason[:100]}...")
            feedback = reason 
            
    print("    > [Mutation] All attempts failed. Fallback to Original Question.")
    return question 

def check_chain_consistency(question: str, step_history: list[str]) -> tuple[bool, str]:
    history_text = "\n".join(step_history)
    messages = [
        {"role": "system", "content": 
            "You are a Final Logic Auditor. Check the solution trace for consistency.\n"
            "Analyze specific errors:\n"
            "1. Common Sense (e.g., negative length, people < 0)\n"
            "2. Premise Conflict (e.g., Question says x+y=10, derived x+y=6)\n"
            "3. Calculation Consistency (Does Step N follow Step N-1?)\n\n"
            "Output Format:\n"
            "STATUS: PASS or FAIL\n"
            "REASON: [If FAIL, describe specific error to help correction]"
        },
        {"role": "user", "content": 
            f"Question: {question}\n\nDerived Solution Path:\n{history_text}\n\n"
            "Is this solution path consistent and valid?"
        }
    ]
    response = safe_generate(messages, temperature=0.0)
    
    if "STATUS: PASS" in response:
        return True, ""
    else:
        reason = response.split("REASON:")[-1].strip() if "REASON:" in response else "Unknown inconsistency"
        return False, reason

def regenerate_step_with_feedback(question: str, previous_steps: list[str], error_feedback: str) -> str:
    history_block = "\n".join(previous_steps)
    messages = [
        {"role": "system", "content": 
            "You are a Robust Math Solver. A previous derivation was detected as INCORRECT.\n"
            "Task: Regenerate the NEXT step, fixing the reported error.\n"
            "Use the Quad Card format."
        },
        {"role": "user", "content": 
            f"Question: {question}\n"
            f"Verified History:\n{history_block}\n\n"
            f"ERROR REPORT: {error_feedback}\n\n"
            "Instruction: Backtrack and generate the correct next logical step (Card A/B/C/D)."
        }
    ]
    return safe_generate(messages, temperature=0.3).strip()

def check_step_validity(mutated_problem: str, previous_verified_steps: list[str], current_step_str: str) -> str:
    card_a_match = re.search(r"Card A: (.*?)(?:\|\||$)", current_step_str)
    card_b_match = re.search(r"Card B: (.*?)(?:\|\||$)", current_step_str)
    card_c_match = re.search(r"Card C: (.*?)(?:\|\||$)", current_step_str)
    card_d_match = re.search(r"Card D: (.*?)(?:\|\||$)", current_step_str)
    
    card_a = card_a_match.group(1).strip() if card_a_match else "N/A"
    card_b = card_b_match.group(1).strip() if card_b_match else "N/A"
    card_c = card_c_match.group(1).strip() if card_c_match else "N/A"
    card_d = card_d_match.group(1).strip() if card_d_match else "N/A"
    
    prev_context = "\n".join(previous_verified_steps)
    
    messages = [
        {"role": "system", "content": 
            "You are a Logic Validator. \n"
            "Task: Select the BEST logic card (A, B, C, or D) that correctly applies to the MUTATED problem.\n"
            "Some cards might rely on old numbers (Invalid). Some are general (Valid)."
        },
        {"role": "user", "content": 
            f"Mutated Problem: {mutated_problem}\n"
            f"History of Verified Steps:\n{prev_context}\n\n"
            f"Candidate Logic A: {card_a}\n"
            f"Candidate Logic B: {card_b}\n"
            f"Candidate Logic C: {card_c}\n"
            f"Candidate Logic D: {card_d}\n\n"
            "Instruction:\n"
            "1. Test each logic candidate on the mutated problem.\n"
            "2. Select the one that works best and produces a valid calculation.\n"
            "3. Output format: 'SELECTED: [A/B/C/D]' or 'FAIL'."
        }
    ]
    
    response = safe_generate(messages, temperature=0.1)
    
    if "FAIL" in response: return None
    if "SELECTED: A" in response: return card_a
    if "SELECTED: B" in response: return card_b
    if "SELECTED: C" in response: return card_c
    if "SELECTED: D" in response: return card_d
    
    if len(response) > 5 and ("=" in response or any(c.isdigit() for c in response)):
         return card_a 
    return None

def run_step_verification_loop(question: str, initial_steps: list[str]) -> list[str]:
    print(f"   - [Verification] Starting Step-by-Step Check (Quad Cards)...")
    
    mutated_q = generate_mutated_variant(question)
    print(f"    > Mutation Scenario: {mutated_q[:60]}...")
    
    final_verified_chain = [] 
    mutated_chain_trace = []  
    
    current_steps_to_process = initial_steps
    max_steps = 10 
    step_idx = 0
    
    while step_idx < len(current_steps_to_process) and step_idx < max_steps:
        raw_step = current_steps_to_process[step_idx]
        original_math_match = re.search(r"Math: (.*)", raw_step)
        original_math = original_math_match.group(1).strip() if original_math_match else ""
        
        valid_card = check_step_validity(mutated_q, mutated_chain_trace, raw_step)
        
        if valid_card:
            print(f"    > Step {step_idx+1}: Verified (Logic OK)")
            final_verified_chain.append(f"Step {step_idx+1}: [Logic: {valid_card}] || [Math: {original_math}]")
            mutated_chain_trace.append(f"Step {step_idx+1}: {valid_card}")
            step_idx += 1
        else:
            print(f"    > Step {step_idx+1}: FAILED Validation. Triggering Regenerate...")
            
            new_step_str = regenerate_step_with_feedback(question, final_verified_chain, "Previous step failed mutation consistency check.")
            
            if "||" in new_step_str:
                print(f"      > Regenerated: {new_step_str[:50]}...")
                current_steps_to_process = current_steps_to_process[:step_idx] + [new_step_str] 
                
                valid_card_retry = check_step_validity(mutated_q, mutated_chain_trace, new_step_str)
                
                if valid_card_retry:
                    new_math = re.search(r"Math: (.*)", new_step_str).group(1).strip()
                    final_verified_chain.append(f"Step {step_idx+1}: [Logic: {valid_card_retry}] || [Math: {new_math}]")
                    mutated_chain_trace.append(f"Step {step_idx+1}: {valid_card_retry}")
                    step_idx += 1
                else:
                    print(f"      > Retry Failed. Aborting logic chain here.")
                    break
            else:
                break
    
    if final_verified_chain:
        is_consistent, error_reason = check_chain_consistency(question, final_verified_chain)
        
        if not is_consistent:
            print(f"    > [Consistency Fail] {error_reason}")
            print(f"    > Triggering Backtrack on Last Step...")
            
            if len(final_verified_chain) > 0:
                final_verified_chain.pop() 
                
                correction = regenerate_step_with_feedback(question, final_verified_chain, error_reason)
                print(f"      > Backtrack Correction: {correction[:50]}...")
                final_verified_chain.append(correction)

    return final_verified_chain

def generate_final_answer(steps: list[str], question: str) -> str:
    if not steps: return "Analysis failed."
    steps_block = "\n".join(steps)
    
    messages = [
        {"role": "system", "content": "You are a Math Solver. Format: Compact LaTeX. End with \\boxed{}."},
        {"role": "user", "content": 
            f"Question: {question}\n"
            f"Verified Logic Path:\n{steps_block}\n\n"
            "Follow the verified logic path to solve the original question. Output final answer in \\boxed{}."
        }
    ]
    return safe_generate(messages, temperature=0.1)

# ============================================================================
# 6. PIPELINE ORCHESTRATOR & CONFIDENCE CHECK
# ============================================================================

def enhanced_logging_pipeline(question: str, baseline_ans: str, gt: str) -> dict:
    results = {
        'question': question,
        'diverse_answers': [],
        'confidence_score': "UNKNOWN",
        'step_cards': [], 
        'final_verified_steps': [],
        'final_answer': "",
        'pipeline_answer': ""
    }
    
    # --- Confidence Check ---
    # Using robust equivalence check
    if is_math_equivalence(baseline_ans, gt) and baseline_ans != "":
        print(f"  [Confidence Check] Baseline matches Ground Truth (Robust Eq). Score: HIGH.")
        print(f"  >> Skipping Mutation Testing (Using Baseline directly).")
        results['confidence_score'] = "HIGH"
        results['pipeline_answer'] = baseline_ans
        results['final_answer'] = f"Confidence High (Matches GT). Used Baseline: {baseline_ans}"
        return results
    else:
        print(f"  [Confidence Check] Baseline mismatch/unknown. Score: LOW.")
        print(f"  >> Activating Quad-Card Mutation Pipeline...")
        results['confidence_score'] = "LOW"

    # --- Pipeline Execution ---
    
    # 1. Generate diverse answers (3 experts)
    answers = generate_diverse_answers(question, n=3)
    results['diverse_answers'] = answers
    
    # 2. Majority Voting to select the best answer
    best_solution_text = select_solution_by_voting(answers)
    
    # 3. Extract Steps (Using the Voted Solution)
    step_objects = extract_steps_with_quad_cards([best_solution_text], question)
    results['step_cards'] = step_objects
    
    # 4. Run Verification (Mutation Check + Consistency Backtrack)
    final_verified_steps = run_step_verification_loop(question, step_objects)
    results['final_verified_steps'] = final_verified_steps
    
    # 5. Final Answer
    final_answer_text = generate_final_answer(final_verified_steps, question)
    pipeline_ans = parse_numeric_answer(final_answer_text)
    
    if not pipeline_ans:
        pipeline_ans = "FALLBACK_PENDING"

    results['final_answer'] = final_answer_text
    results['pipeline_answer'] = pipeline_ans
    
    return results

def save_detailed_analysis(results: dict, question_index: int, results_dir: str):
    analysis_file = f"{results_dir}/pipeline_analysis_{question_index}.json"
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)

# ============================================================================
# 7. MAIN LOOP - MODIFIED
# ============================================================================

def main():
    results_dir = "./results"
    os.makedirs(results_dir, exist_ok=True)
    
    print("Loading AMC23 dataset...")
    _, records = load_real_math_dataset()
    
    # --- 修改点: 索引从 0 开始 ---
    start_idx = 30 
    end_idx = 41 # 跑完全部数据
    # 如果只是测试，可以设为: end_idx = 10 
    
    problems_to_run = records[start_idx:end_idx]
    
    total = len(problems_to_run)
    print(f"Evaluating {total} problems (Index {start_idx} to {end_idx})")
    
    base_correct = 0
    pipe_correct = 0
    index = 0
    
    open(f"{results_dir}/detailed_results.txt", "w").close()
    
    for item in tqdm(problems_to_run, desc="Evaluating"):
        index += 1
        q = item.get("problem", "")
        # GT from dataset mapping
        gt = parse_numeric_answer(item.get("solution", ""))
        
        print(f"\n{'='*80}\nPROBLEM {index}/{total}\n{'='*80}")
        print(f"Question: {q[:100]}...")
        print(f"Ground Truth: {gt}")
        
        # --- 1. Baseline Phase ---
        print(f"--- Generating Baseline ---")
        baseline_raw = generate_baseline_answer(q)
        base_ans = parse_numeric_answer(baseline_raw)
        if not base_ans: base_ans = "0"
        
        # Robust Check
        is_base_correct = is_math_equivalence(base_ans, gt)
        base_correct += int(is_base_correct)
        print(f"Baseline Answer: {base_ans} | Correct: {'✓' if is_base_correct else '✗'}")
        
        # --- 2. Pipeline Phase ---
        pipe_ans = ""
        is_pipe_correct = False
        pipeline_note = ""
        
        try:
            results = enhanced_logging_pipeline(q, base_ans, gt)
            pipe_ans = results['pipeline_answer']
            
            if results['confidence_score'] == "HIGH":
                pipeline_note = "High Conf (Skipped)"
            else:
                pipeline_note = "Low Conf (Verified)"
                if pipe_ans == "FALLBACK_PENDING" or pipe_ans == "":
                    print(f"  [FALLBACK] Pipeline yielded no result. Reverting to Baseline.")
                    pipe_ans = base_ans
                    pipeline_note += " -> Fallback"
            
            if not pipe_ans: pipe_ans = "0"
            
            # Robust Check
            is_pipe_correct = is_math_equivalence(pipe_ans, gt)
            pipe_correct += int(is_pipe_correct)
            print(f"Pipeline Answer: {pipe_ans} | Correct: {'✓' if is_pipe_correct else '✗'}")
            
            save_detailed_analysis(results, index, results_dir)
            
        except Exception as e:
            print(f"Pipeline failed: {e}")
            import traceback
            traceback.print_exc()
            pipe_ans = base_ans
            is_pipe_correct = is_base_correct 
            pipeline_note = "Crashed"

        # --- Stats ---
        print(f"\n--- Running Statistics ({index}/{total}) ---")
        print(f"Baseline Correct: {base_correct}/{index} ({100*base_correct/index:.1f}%)")
        print(f"Pipeline Correct: {pipe_correct}/{index} ({100*pipe_correct/index:.1f}%)")
        print(f"Net Gain: +{pipe_correct - base_correct}")
        
        with open(f"{results_dir}/detailed_results.txt", "a", encoding="utf-8") as out:
            out.write(f"Q{index}: GT={gt} | Base={base_ans} ({is_base_correct}) | Pipe={pipe_ans} ({is_pipe_correct}) [{pipeline_note}]\n")
            
        if index % 2 == 0:
            torch.cuda.empty_cache()
            gc.collect()

    print(f"\n{'='*80}\nFINAL RESULTS\n{'='*80}")
    print(f"Total Problems: {total}")
    print(f"Baseline Accuracy: {base_correct}/{total} ({100*base_correct/total:.1f}%)")
    print(f"Pipeline Accuracy: {pipe_correct}/{total} ({100*pipe_correct/total:.1f}%)")
    if total > 0:
        print(f"Improvement: +{pipe_correct - base_correct} solved problems")

    print(f"Results saved to: {results_dir}/")

if __name__ == '__main__':
    main()